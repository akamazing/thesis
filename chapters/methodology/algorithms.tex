\section{Implementation of machine learning algorithms}\label{Algorithms}
In order to answer research questions two and three, we implemented some popular machine learning algorithms to provide quantitative data on the effect of metamorphic relations on the accuracy of datasets and their classes. We used TensorFlow and scikit-learn to implement the following machine learning algorithms.

\subsection{Convolutional neural network}
The images in the MNIST dataset is a one-dimensional vector of 784 features (28x28 pixels). We first reshape the images to match the picture format height x width x channel. Each image in the MNIST dataset is now a single-channel of 28x28x1 size. The images are then fed into the network as input. We used tensorflow to implement the convolutional network. The first convolution layer has 32 filters with a kernel size of five. The activation function we used is the ReLU function, and the output from this layer is fed into a pooling layer. The data goes through another iteration of convolution and pooling before being flattened and sent to the fully connected layer. The output layer gives us a number that corresponds to the class of the image it is assigned.

\subsection{Neural network}
We implemented a neural network with seven hidden layers using TensorFlow. The input image is a one-dimensional vector of 784 features (28x28 pixels), where each pixel value lies between $0.0$ and $1.0$. The pixel value represents $0.0$ white, and the value $1.0$ represents black, and all the values in between represent varying degrees of grey. Each hidden layer is made up of 256 neurons and is fully connected to its previous layer. All the neurons in a layer are independent, and none of them share a connection with each other. The structure of the network was determined empirically by testing multiple numbers of layers and picking a network that produced the best accuracy. 
The output layer is a fully-connected layer with one neuron for each class. The output layer produces a value between 0 and 1 for each class. A value of less than 0.5 indicates that the input image does not belong to that class, and a value greater than 0.5 indicates that the image belongs to that class. The image is assigned to the class of the neuron with maximum value. 

%  Hidden fully connected layer with 256 neurons. Each hidden layer is made up of a set of neurons, where each neuron is fully connected to all neurons in the previous layer, and where neurons in a single layer function completely independently and do not share any connections. If the image is a 64 by 64 greyscale image, then we'd have 64x64=4,096 input neurons, with the intensities scaled appropriately between 0 and 1. The output layer will contain just a single neuron, with output values of less than 0.5 indicating "input image is not a 9", and values greater than 0.5 indicating "input image is a 9 ". The input pixels are greyscale, with a value of 0.0 representing white, a value of 1.0 representing black, and in between values representing gradually darkening shades of grey. The second layer of the network is a hidden layer.


\subsection{Naive Bayes}
We used scikit-learn to implement the Naive Bayes algorithm. We transformed the images into a one-dimensional vector of 784 features. 
Using the $sklearn.naive\_bayes.MultinomialNB()$ function we created a Naive Bayes classifier, which is then fitted using the $fit()$ function, which takes the images and labels as input and fits the classifier according to the images and labels. The resulting classifier is then used to predict the labels of test data.

\subsection{$k$-Nearest Neighbor}
 We used scikit-learn to implement our $k$-nearest neighbor algorithm. Scikit's $sklearn.neighbors$ module provides the functionality to implement unsupervised and supervised neighbors-based learning methods. $k$ is an integer provided by the user.
 The KNeighborsClassifier function from the $sklearn.neighbors$ module implements learning based on the $k$ nearest neighbors of each query point. The k-neighbors classification in KNeighborsClassifier is the most commonly used technique. Since the optimal value $k$ is dependent on data, we ran the experiment with multiple values of $k$ and found that the $k$=3 produces the best accuracy. In general, a larger $k$ makes the classification boundaries less distinct but suppresses the effects of noise.
 We used Euclidean distance as the distance metric and assigned equal weights to all the query points. In some cases, it might be better to assign weights based on the distance. The close query point contributes more to the fit, and the ones far contribute less. For our case, the uniform weight with a simple majority vote of the nearest neighbors worked just fine.

\subsection{SVM}
We also implemented the support vector machine algorithm using scikit-learn. The $sklearn.svm.SVC$ class is based on libsvm and is capable of performing multi-class classification. Internally, they use libsvm and liblinear to handle all computations. These libraries are wrapped using C and Cython. It supports multi-class classification on a one-vs-one scheme. In this scheme, a classifier is created for each pair of classes and during prediction, the class which receives most votes is assigned to the data point. In the event of a tie, we selected the class with the highest total confidence calculated by summing over the pair-wise classification confidence. 
As the other classifiers, $skleran.SVM$ also takes as input two arrays: an array of training samples and another array of class labels. The training samples and labels are passed to $fit$ function, which fits the SVM model according to the data. The resulting model is then used to predict the labels of test data. 


After generating the follow-up test cases and implementing the machine-learning algorithms, we tested the trained model on the five follow-up test cases. The rotate, shade, and shear transformations each have 100 versions of the transformed original dataset. The shiftX and shiftY transformations each have 56 versions of original test data as follow-up test data. Each of the 412 follow-up test datasets was then tested on the trained models, and their accuracy was calculated. We will discuss our findings in the next section.

