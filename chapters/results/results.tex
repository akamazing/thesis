In this section we will discuss our findings after implementing the machine learning algorithms and the metamorphic properties.

Accuracy is defined as the number of correct predictions made by the model with respect to the total number of predictions made. To evaluate the accuracy of our machine learning models, we first trained the models on the training data. The MNIST, Fashion-MNIST, and, EMNIST training datasets have 55000, 55000, and, 119800 training data respectively. The training data consists of the images and their corresponding labels which are used as input to the algorithm in order to generate a model. Once we generated the trained models, we used the test data to calculate the accuracy of the models. The MNIST, Fashion-MNIST, and, EMNIST testing dataset have 10000, 10000, and, 20800 testing data respectively. The following graphs show the accuracy of the various models on the test dataset.

\pgfplotstableread[row sep=\\,col sep=&]{
    interval & MNIST & Fashion & EMNIST \\
    CNN     & 97.01 & 83.56 & 78.20 \\
    NN     & 91.41 & 79.89 & 64.96 \\
    NB    & 80.85 & 65.6 & 57.81 \\
    KNN   & 97.28 & 85.24 & 86.25 \\
    SVM   & 95.47 & 84.56 & 81.25\\
    }\accuracy
    
\begin{center}
\begin{tikzpicture}
    \begin{axis}[
            ybar,
            bar width=.75cm,
            height=.5\textwidth,
            width=\textwidth,
            legend style={at={(0.5,1)},
                anchor=north,legend columns=-1},
            symbolic x coords={CNN,NN,NB,KNN, SVM},
            xtick=data,
            ytick={0,20,40,60,80,100},
            nodes near coords,
            nodes near coords align={vertical},
            ymin=0,ymax=120,
            ylabel={Accuracy},
            xlabel={Algorithms}
        ]
        \addplot table[x=interval,y=MNIST]{\accuracy};
        \addplot table[x=interval,y=Fashion]{\accuracy};
        \addplot table[x=interval,y=EMNIST]{\accuracy};
        \legend{MNIST, Fashion, EMNIST}
    \end{axis}
\end{tikzpicture}      
\end{center}{}
An interesting observation from the table above is that, deep learning algorithms are not necessarily always a good choice when we are trying to maximize accuracy. Depending on the type of dataset, regular machine learning algorithms like KNN can also yield high accuracy and outperform deep learning algorithms.
In the following sections we will discuss how the metamorphic transformations affected the accuracy of the models and what conclusions we can draw from it.
\section{Effect of metamorphic transformation on the accuracy of the model}
From literature review we saw that metamorphic testing can be a very useful technique to test the correctness of an algorithm. In this thesis, we will investigate the effect of the identified metamorphic properties on the accuracy of the machine learning algorithms. For each metamorphic property identified in \ref{identifyingMR} we want to see how the accuracy of the models change with the varying degree of transformation. Our hypothesis is that small changes to the input image should not affect the prediction of the model drastically. After creating a trained model with the training dataset, we applied the metamorphic properties on the training data to generate transformed versions of the test data. We calculated the following metrics to evaluate the effect of the metamorphic relations on machine learning algorithm. To tackle the stochastic nature of training a neural network model, we ran each of the above experiment ten times and calculated average for accuracy.

\input{figures.tex}

\clearpage
\subsection{Robustness}

In the section \ref{Digit-by Accuracy} we saw that that the accuracy of the algorithms varied depending upon the transformation applied to the test data as well as the degree of the transformation. Robustness is the property of an algorithm whereby if the test data is "similar" to the training data then the testing error is also close to the training error \cite{Xu2012}. Thus, a robust algorithm should produce similar errors while predicting similar test data. In this section, we will be comparing the robustness of the five algorithms with respect to each other for different metamorphic relations and verify the accuracy of the recommendations we made in the table \ref{tbl:indexrecommendations}. For this we generated a new dataset with those recommendations and compared it's accuracy with the accuracy of the original dataset. To generate a balanced dataset of 5000 samples we randomly picked 100 transformed images from each transformation for every class label. This new test dataset was then tested for accuracy on the five algorithms we implemented. The bar graph below shows the accuracy of the new test dataset as compared to the original MNIST dataset.

\pgfplotstableread[row sep=\\,col sep=&]{
    Algorithm & MNIST & Transformed  \\
    CNN     & 97.01 & 95.2 \\
    NN     & 91.41 & 89.02 \\
    NB    & 80.85 & 78.76 \\
    SVM   & 95.47 & 90.98\\
    KNN   & 97.28 & 93.06\\
    }\Robustness
    
\begin{figure}[H]
\begin{tikzpicture}
    \begin{axis}[
            ybar,
            bar width=.75cm,
            height=.5\textwidth,
            width=\textwidth,
            legend style={at={(0.5,1)},
                anchor=north,legend columns=-1},
            symbolic x coords={CNN,NN,NB,KNN, SVM},
            xtick=data,
            ytick={0,20,40,60,80,100},
            nodes near coords,
            nodes near coords align={vertical},
            ymin=0,ymax=120,
            ylabel={Accuracy},
            xlabel={Algorithms}
        ]
        \addplot table[x=Algorithm,y=MNIST]{\Robustness};
        \addplot table[x=Algorithm,y=Transformed]{\Robustness};
        \legend{MNIST, Transformed Images}
    \end{axis}
\end{tikzpicture}      
\caption{Accuracy of tranformed dataset vs original dataset}
\label{img:transformedaccuracy}
\end{figure}

The more robust algorithm will have the accuracy of the transformed images closer to the accuracy of the original MNIST data. From the graph above we see that the convolutional neural network has a $1.81\%$ decrease in accuracy while the accuracy of the neural-network, NB, KNN, and, SVM decreased by $2.39\%$, $2.09\%$, $4.22\%$, and, $4.49\%$ respectively. Thus, among the algorithms we implemented the convolutional neural network is proven to be the most robust followed by Naive Bayes implementation. While the KNN and SVM implementations are the least robust.

In Section \ref{4.1.1} we discovered that the accuracy of the test data decreased as the degree of the transformation was increased. So a second way of looking at robustness is to look at transformations which can be applied to the dataset without lowering the accuracy of the model below 90 percent of the original value.




\import{chapters/results/}{tables.tex}

\newpage
\section{Discussion}
In this paper we identified five metamorphic properties "Rotation", "Shading", "Shearing", "Shifting the image along $x$ axis", and, "Shifting the image along $y$ axis" that can be used with image datasets. We then applied these metamorphic transformations to three datasets: MNIST, Fashion-MNIST, and, EMNIST. We also looked at how the accuracy of the datasets are affected by the transformations we identified. We also investigated the accuracy of transformations on individual classes in the MNIST dataset. We then provide recommendations for creating follow up test cases from the MNIST dataset by using the minimum and maximum values of transformations to use for these metamorphic properties. We also implemented five common machine learning algorithms and evaluated their robustness on these metamorphic properties.
We make following recommendations for transforming digits in MNIST dataset.
This paper also make the following contributions: 
We made a methodological contribution where we have provided a set of functions to easily manipulate, and, transform image datasets for future research. Future researchers can utilize these functions to plugin more algorithms and metamorphic properties.

Contributions:
Methodological contribution where others can plugin more algors or MT.
Experimental contribution:Which algo does better.


Conclusions:

Framework to apply MT in midst of distortions.
CNN not so much better than others based on learning just from the dataset.

